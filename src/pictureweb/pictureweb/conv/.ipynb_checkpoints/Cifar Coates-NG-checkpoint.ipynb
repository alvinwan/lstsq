{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vaishaal/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['imread']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import imagenet_load\n",
    "from importlib import reload\n",
    "reload(imagenet_load)\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import imread\n",
    "%pylab inline\n",
    "import imread\n",
    "from imread import imread_from_blob\n",
    "from imagenet_load import orient\n",
    "import multigpu\n",
    "reload(multigpu)\n",
    "import filter_gen\n",
    "import conv\n",
    "reload(conv)\n",
    "import gc\n",
    "import logging\n",
    "import dill\n",
    "from scipy import linalg\n",
    "from sklearn import metrics\n",
    "import opt\n",
    "import copy\n",
    "reload(opt)\n",
    "reload(filter_gen)\n",
    "import math\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cifar_processed():\n",
    "    npzfile = np.load(\"./cifar_processed\")\n",
    "    return (npzfile['XTrain'], npzfile['yTrain']), (npzfile['XTest'], npzfile['yTest'])\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_exp(K):\n",
    "    for i in range(K.shape[0]):\n",
    "        for j in range(K.shape[1]):\n",
    "            K[i,j] = math.exp(K[i,j])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(XTrain, labelsTrain), (XTest, labelsTest) = load_cifar_processed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((XTrain, XTest), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_handler = multigpu.MultiGpuHandler(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 4: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 14: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 15: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 12: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 1: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 5: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 7: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 2: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 6: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 3: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 11: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 8: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 10: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 9: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "Using gpu device 13: Tesla K80 (CNMeM is disabled, cuDNN 5105)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "gpu_handler.start_all()\n",
    "gpu_handler.wait_for_all_gpu_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patches = filter_gen.patchify_all_imgs(XTrain, (6,6), pad=False)\n",
    "\n",
    "patches = patches.reshape(patches.shape[0]*patches.shape[1],*patches.shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patches_mmap_loc = \"/dev/shm/patches_cifar\"\n",
    "patches_mmap_data = np.memmap(patches_mmap_loc, mode=\"w+\", shape=patches.shape, dtype=patches.dtype)\n",
    "np.copyto(patches_mmap_data, patches)\n",
    "\n",
    "patches_mmap_data.flush()\n",
    "patches_mmap = multigpu.MmapArray(patches_mmap_data, mode=\"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fg = filter_gen.make_empirical_filter_gen(patches_mmap, 1e-4)\n",
    "\n",
    "conv_args_template = \\\n",
    "{ \n",
    "  \"filter_gen\":fg,\n",
    "  \"num_feature_batches\":4,\n",
    "  \"data_batch_size\":1024,\n",
    "  \"feature_batch_size\":2048,\n",
    "  \"pool_size\":15,\n",
    "  \"pool_type\":\"avg\",\n",
    "  \"pool_stride\":6,\n",
    "  \"patch_size\":6,\n",
    "  \"pad\":0,\n",
    "  \"bias\":1,\n",
    "  \"conv_stride\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_mmap_data = np.memmap(\"/dev/shm/cifar\", mode=\"w+\", dtype=\"float32\", shape=X.shape)\n",
    "np.copyto(X_mmap_data, X)\n",
    "X_mmap = multigpu.MmapArray(X_mmap_data, mode=\"r+\")\n",
    "\n",
    "mmap_out_shape = conv.conv_compute_output_shape(data=X_mmap, **conv_args_template)\n",
    "X_out_data = np.memmap(\"/dev/shm/cifar_features\", mode=\"w+\", dtype=\"float32\", shape=mmap_out_shape)\n",
    "X_out_mmap = multigpu.MmapArray(X_out_data, mode=\"r+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 16384, 3, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_out_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk_idxs(size, chunks):\n",
    "    chunk_size  = int(np.ceil(size/chunks))\n",
    "    idxs = list(range(0, size+1, chunk_size))\n",
    "    if (idxs[-1] != size):\n",
    "        idxs.append(size)\n",
    "    return list(zip(idxs[:-1], idxs[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_chunk_idxs = chunk_idxs(X.shape[0], 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 24 ms, total: 56 ms\n",
      "Wall time: 18.3 s\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 250 ms\n",
      "CPU times: user 0 ns, sys: 8 ms, total: 8 ms\n",
      "Wall time: 2.78 s\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 116 ms\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 191 ms\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 1.53 s\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.93 s\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 193 Âµs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.16 s\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.41 s\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.3 s\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 640 ms\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 746 ms\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.34 s\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.79 s\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 623 ms\n"
     ]
    }
   ],
   "source": [
    "gpu_results = []\n",
    "for gpu, (sidx, eidx) in list(zip(gpu_handler.gpus, data_chunk_idxs)):\n",
    "        conv_args = conv_args_template.copy()\n",
    "        X_mmap_gpu = copy.copy(X_mmap)\n",
    "        X_mmap_gpu.idxs = (sidx, eidx)\n",
    "        X_out_gpu = copy.copy(X_out_mmap)\n",
    "        X_out_gpu.idxs = (sidx, eidx)\n",
    "        conv_args['data'] = X_mmap_gpu\n",
    "        conv_args['output'] = X_out_gpu\n",
    "        gpu_result = gpu.submit_async(conv.conv_mmap, **conv_args)\n",
    "        gpu_results.append(gpu_result)\n",
    "\n",
    "for gpu_result in gpu_results:\n",
    "    %time gpu_result.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_out_data.reshape(X_out_data.shape[0], -1)[:50000, :]\n",
    "X_test = X_out_data.reshape(X_out_data.shape[0], -1)[50000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_feats = conv_args_template[\"num_feature_batches\"]*conv_args_template[\"feature_batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train /= np.sqrt(num_feats)\n",
    "X_test /= np.sqrt(num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 147456)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 27min 22s, sys: 20min 47s, total: 1h 48min 9s\n",
      "Wall time: 3min 31s\n",
      "CPU times: user 36min 8s, sys: 10min 56s, total: 47min 4s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%time K_train = X_train.dot(X_train.T)\n",
    "%time K_test =  X_test.dot(X_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Dual Model\n",
      "reg is 0.001\n",
      "(0.99970000000000003, 0.43640000000000001)\n",
      "Learning Dual Model\n",
      "reg is 0.01\n",
      "(0.99875999999999998, 0.81240000000000001)\n",
      "Learning Dual Model\n",
      "reg is 0.1\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for reg in [1e-3,1e-2,1e-1]:\n",
    "    acc = opt.trainAndEvaluateDualModel(KTrain=K_train, KTest=K_test, labelsTrain=labelsTrain, labelsTest=labelsTest, reg=reg)\n",
    "    print(acc)\n",
    "    results[reg] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[ 33.68572998,  36.69067764,  15.68627167, ...,  18.12153435,\n",
       "         25.56963348,  30.01623154],\n",
       "       [ 36.69067764,  45.02524948,  17.93997955, ...,  20.81598091,\n",
       "         30.85154915,  35.13890076],\n",
       "       [ 15.68627167,  17.93997955,   9.18136597, ...,   9.89673138,\n",
       "         11.17640495,  15.0761385 ],\n",
       "       ..., \n",
       "       [ 18.12153435,  20.81598091,   9.89673138, ...,  12.07184887,\n",
       "         13.21132946,  17.49361801],\n",
       "       [ 25.56963348,  30.85154915,  11.17640495, ...,  13.21132946,\n",
       "         25.44717979,  24.90920258],\n",
       "       [ 30.01623154,  35.13890076,  15.0761385 , ...,  17.49361801,\n",
       "         24.90920258,  31.33226776]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norms_train = np.linalg.norm(X_train, axis=1)\n",
    "norms_test = np.linalg.norm(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K_train_rbf = K_train.copy()\n",
    "K_test_rbf = K_test.copy()\n",
    "norms_square_train = np.square(norms_train)[:, np.newaxis]\n",
    "norms_square_test = np.square(norms_test)[:, np.newaxis]\n",
    "K_train_rbf *= -2\n",
    "K_test_rbf *= -2 \n",
    "K_train_rbf += norms_square_train.T \n",
    "K_train_rbf += norms_square_train\n",
    "K_test_rbf += norms_square_test\n",
    "K_test_rbf += norms_square_train.T\n",
    "K_test_rbf *= -0.0025 \n",
    "K_train_rbf *= -0.0025 \n",
    "K_train_rbf = fast_exp(K_train_rbf)\n",
    "K_test_rbf = fast_exp(K_test_rbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Dual Model\n",
      "reg is 1e-06\n",
      "(1.0, 0.82750000000000001)\n",
      "Learning Dual Model\n",
      "reg is 1e-05\n",
      "(1.0, 0.83689999999999998)\n",
      "Learning Dual Model\n",
      "reg is 0.0001\n",
      "(0.99912000000000001, 0.84960000000000002)\n",
      "Learning Dual Model\n",
      "reg is 0.001\n",
      "(0.94796000000000002, 0.82820000000000005)\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for reg in (1e-6, 1e-5, 1e-4, 1e-3):\n",
    "    acc = opt.trainAndEvaluateDualModel(KTrain=K_train_rbf, KTest=K_test_rbf, labelsTrain=labelsTrain, labelsTest=labelsTest, reg=reg)\n",
    "    print(acc)\n",
    "    results[reg] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "print(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
